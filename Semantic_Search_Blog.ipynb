{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Search Engine Using NLP and BERT\n",
    "## By [Your Name Here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Motivation\n",
    "We are surrounded by massive amounts of textual data, and retrieving relevant information quickly is a common need across industries \u2014 from customer support to academic research. Traditional keyword-based search engines often fail to capture the semantic meaning behind a query. This project addresses that by building a semantic search engine using modern NLP techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Connection to Multimodal Learning\n",
    "While this project focuses on text, the progression from Bag-of-Words to BERT embeddings reflects a deepening multimodal understanding \u2014 going from sparse symbolic features to dense contextual embeddings. It also mirrors the evolution in NLP pipelines: \n",
    "- Classic representations (BoW, TF-IDF)\n",
    "- Learned word embeddings (Word2Vec)\n",
    "- Deep contextual representations (BERT)\n",
    "K-Means clustering is applied only in the final stage (after BERT), showing how we can organize meaningfully similar documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Learnings from This Work\n",
    "- Different vectorization methods capture different levels of semantic information\n",
    "- Cosine similarity is a powerful tool to compare vectors\n",
    "- Pretrained BERT embeddings outperform previous methods in understanding query context\n",
    "- K-Means clustering helps organize semantically similar documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "documents = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"How to cook pasta?\",\n",
    "    \"Where is Paris located?\",\n",
    "    \"Recipe for Italian pasta\",\n",
    "    \"Best tourist spots in France\"\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(documents)\n",
    "sim_matrix_bow = cosine_similarity(bow_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "sim_matrix_tfidf = cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "tokenized_docs = [doc.lower().split() for doc in documents]\n",
    "model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=1, workers=2)\n",
    "def document_vector(doc):\n",
    "    doc = doc.lower().split()\n",
    "    return np.mean([model.wv[word] for word in doc if word in model.wv], axis=0)\n",
    "w2v_matrix = np.vstack([document_vector(doc) for doc in documents])\n",
    "sim_matrix_w2v = cosine_similarity(w2v_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT + KMeans\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "bert_embeddings = model.encode(documents)\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans.fit(bert_embeddings)\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying\n",
    "query = \"Places to visit in Europe\"\n",
    "query_embedding = model.encode([query])\n",
    "sim_scores = cosine_similarity(query_embedding, bert_embeddings)[0]\n",
    "top_doc_index = np.argmax(sim_scores)\n",
    "print(\"Top Match:\", documents[top_doc_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reflections\n",
    "### (a) What surprised me:\n",
    "- Word2Vec sometimes performs well even with simple models\n",
    "- BERT models require minimal tuning to perform powerfully out of the box\n",
    "### (b) Improvements\n",
    "- Add visualizations (e.g. t-SNE or PCA)\n",
    "- Use a larger dataset with evaluation metrics (e.g. MAP, NDCG)\n",
    "- Incorporate cross-modal search (image + text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. References\n",
    "- https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "- https://scikit-learn.org/\n",
    "- https://radimrehurek.com/gensim/\n",
    "- https://nlp.stanford.edu/projects/glove/\n",
    "- https://bair.berkeley.edu/blog/2024/07/20/visual-haystacks/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}